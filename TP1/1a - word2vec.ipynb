{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Vectorización\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['que', 'dia', 'es', 'hoy'], ['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes'], ['martes', 'muchas', 'gracias']]\n","['que', 'dia', 'es', 'hoy', 'martes', 'el', 'de', 'muchas', 'gracias']\n"]}],"source":["def get_corpus_lists(corpus_input):\n","    term_list = list()\n","    unique_term_list = list()\n","    for document in corpus_input:\n","        terms = list(document.split(' '))\n","        term_list.append(terms)\n","        for term in terms:\n","            if term not in unique_term_list:\n","                unique_term_list.append(term)\n","    return term_list, unique_term_list\n","\n","term_list, unique_term_list = get_corpus_lists(corpus)\n","\n","print(term_list)\n","print(unique_term_list)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Dada una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[{"data":{"text/plain":["array([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n","       [0, 1, 1, 1, 1, 1, 1, 0, 0],\n","       [0, 0, 0, 0, 1, 0, 0, 1, 1]], dtype=uint8)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["def text_list_to_OHE(input_document_list, input_unique_term_list):\n","    return_array = np.zeros([len(input_document_list), len(input_unique_term_list)], dtype=np.uint8)\n","    for index_document, document in enumerate(input_document_list):\n","        for term in document:\n","            try:\n","                unique_index = input_unique_term_list.index(term)\n","            except ValueError:\n","                unique_index = -1\n","            if unique_index != -1:\n","               return_array[index_document][unique_index] = 1\n","\n","    return return_array\n","\n","text_list_to_OHE(term_list, unique_term_list)"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"data":{"text/plain":["array([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n","       [0, 1, 1, 1, 2, 1, 1, 0, 0],\n","       [0, 0, 0, 0, 1, 0, 0, 1, 1]], dtype=uint8)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["def text_list_to_freq(input_document_list, input_unique_term_list):\n","    return_array = np.zeros([len(input_document_list), len(input_unique_term_list)], dtype=np.uint8)\n","    for index_document, document in enumerate(input_document_list):\n","        for term in document:\n","            try:\n","                unique_index = input_unique_term_list.index(term)\n","            except ValueError:\n","                unique_index = -1\n","            if unique_index != -1:\n","               return_array[index_document][unique_index] = return_array[index_document][unique_index] + 1\n","\n","    return return_array\n","\n","text_list_to_freq(term_list, unique_term_list)"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"data":{"text/plain":["array([[0.47712125, 0.17609126, 0.17609126, 0.17609126, 0.        ,\n","        0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.17609126, 0.17609126, 0.17609126, 0.35218252,\n","        0.47712125, 0.47712125, 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.17609126,\n","        0.        , 0.        , 0.47712125, 0.47712125]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["def text_list_to_TF_IDF(input_document_list, input_unique_term_list):\n","    #return_array = np.zeros([1, len(input_unique_term_list)], dtype=np.uint8)\n","    OHE_array = text_list_to_OHE(input_document_list, input_unique_term_list)\n","    IDF = np.log10(len(input_document_list) / np.sum(OHE_array, axis = 0))\n","    TF = text_list_to_freq(input_document_list, input_unique_term_list)\n","    TF_IDF = TF * IDF\n","    return TF_IDF\n","\n","text_list_to_TF_IDF(term_list, unique_term_list)"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[{"data":{"text/plain":["array(['martes muchas gracias', 'que dia es hoy',\n","       'martes el dia de hoy es martes'], dtype='<U30')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def cosine_sim_order(input_corpus, input_document_index):\n","    cosine_sim = np.zeros([len(input_corpus)])\n","    documents, unique_terms = get_corpus_lists(input_corpus)\n","    TF_IDF = text_list_to_TF_IDF(documents, unique_terms)\n","    TF_IDF_selected = TF_IDF[input_document_index]\n","    for document_index, document in enumerate(corpus):\n","        cosine_sim[document_index] = cosine_similarity(TF_IDF[document_index], TF_IDF_selected)\n","    sort = np.argsort(cosine_sim)\n","\n","    return input_corpus[sort]\n","\n","cosine_sim_order(corpus, 1)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
